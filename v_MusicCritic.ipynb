{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4540d9e-f4b7-4fcb-8a8e-d9b30c0800a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b245168-2ed5-43c0-9a2b-ead69f762514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "USEABLE_KEYS = [i+\":\" for i in \"BCDFGHIKLMmNOPQRrSsTUVWwXZ\"]\n",
    "\n",
    "\n",
    "def read_abc(path):\n",
    "    keys = []\n",
    "    notes = []\n",
    "    with open(path) as rf:\n",
    "        for line in rf:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"%\"):\n",
    "                continue\n",
    "\n",
    "            if any([line.startswith(key) for key in USEABLE_KEYS]):\n",
    "                keys.append(line)\n",
    "            else:\n",
    "                notes.append(line)\n",
    "\n",
    "    keys = \" \".join(keys)\n",
    "    notes = \"\".join(notes).strip()\n",
    "    notes = notes.replace(\" \", \"\")\n",
    "\n",
    "    if notes.endswith(\"|\"):\n",
    "        notes = notes[:-1]\n",
    "\n",
    "    notes = notes.replace(\"[\", \" [\")\n",
    "    notes = notes.replace(\"]\", \"] \")\n",
    "    notes = notes.replace(\"(\", \" (\")\n",
    "    notes = notes.replace(\")\", \") \")\n",
    "    notes = notes.replace(\"|\", \" | \")\n",
    "    notes = notes.strip()\n",
    "    notes = \" \".join(notes.split(\" \"))\n",
    "    \n",
    "    if not keys or not notes:\n",
    "        return None, None\n",
    "\n",
    "    return keys, notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb345d1-f8ea-48ca-acdc-f85a91c1a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import youtokentome as yttm\n",
    "\n",
    "train_dir = \"./yandex-music-generation-contest/cleaned_data\"\n",
    "\n",
    "train_paths = list(Path(train_dir).glob(\"*.abc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b350460-2919-46d4-b13d-f362109ec50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE_MODEL_FILENAME = './models/abc_bpe.yttm'\n",
    "TRAIN_TEXTS_FILENAME = './datasets/abc_bpe_train.txt'\n",
    "# with open(TRAIN_TEXTS_FILENAME, \"w\") as f:\n",
    "#     for file in tqdm(train_paths):\n",
    "#         (keys, notes) = read_abc(file)\n",
    "#         f.write(f\"{notes}\\n\")\n",
    "\n",
    "# yttm.BPE.train(data=TRAIN_TEXTS_FILENAME, vocab_size=500, model=BPE_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eb6a2e-0c53-4574-851e-2d3a4a118ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(BPE_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606587b3-4018-44c0-a8fa-aab0c2c65ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 511895/511895 [03:58<00:00, 2142.89it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "for i, p in enumerate(tqdm(train_paths)):\n",
    "    (keys, notes) = read_abc(p)\n",
    "    if keys is None:\n",
    "        continue\n",
    "\n",
    "    keys_tokens = tokenizer.encode(keys)\n",
    "    bars = notes.split(\" | \")\n",
    "    notes_tokens = [tokenizer.encode(i + \" | \") for i in bars]\n",
    "\n",
    "    ## To avoid OOM\n",
    "    sequence_len = sum(len(i) for i in notes_tokens)\n",
    "    if (16 > sequence_len > 256):\n",
    "        print(\"Skip\", p)\n",
    "        continue\n",
    "            \n",
    "    if i % 5 == 0:\n",
    "        test_data.append((keys_tokens, notes_tokens))\n",
    "    else:\n",
    "        train_data.append((keys_tokens, notes_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef81af2-bfaf-41ac-bc00-22b89c643614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ABCDataset(Dataset):\n",
    "    def __init__(self, data,\n",
    "                 context_bars_num=8, \n",
    "                 target_bars_num=8,\n",
    "                 bos_id=2,\n",
    "                 eos_id=3,\n",
    "                 pad_id=0,\n",
    "                 is_test=False,\n",
    "                 max_len = 2048):\n",
    "        \n",
    "        self.notes = []\n",
    "        self.keys = []\n",
    "\n",
    "        for (keys, notes) in data:\n",
    "            if notes is None:\n",
    "                continue\n",
    "\n",
    "            self.keys.append(keys)\n",
    "            self.notes.append(notes)\n",
    "            \n",
    "        self.context_bars_num = context_bars_num\n",
    "        self.target_bars_num = target_bars_num\n",
    "        self.bos_id = bos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        notes = self.notes[idx]\n",
    "        keys = self.keys[idx]\n",
    "        \n",
    "        if not self.is_test:\n",
    "            split_indx = 8\n",
    "\n",
    "            # split notes to context (input for network) and target (that model must to generate)\n",
    "            context_notes = notes[split_indx - self.context_bars_num : split_indx]\n",
    "            target_notes = notes[split_indx: split_indx + self.target_bars_num]\n",
    "        else:\n",
    "            context_notes = notes\n",
    "            target_notes = []\n",
    "\n",
    "        sequence = []\n",
    "\n",
    "        for bar in context_notes:\n",
    "            sequence += bar\n",
    "\n",
    "        for bar in target_notes:\n",
    "            sequence += bar\n",
    "        \n",
    "        if len(sequence) < self.max_len:\n",
    "            sequence += [0] * (self.max_len - len(sequence))\n",
    "        context_tokens = sequence[:-1]\n",
    "        target_tokens = sequence[1:]\n",
    "\n",
    "        context_tokens = torch.tensor(context_tokens, dtype=torch.long)\n",
    "        target_tokens = torch.tensor(target_tokens, dtype=torch.long)\n",
    "\n",
    "        return {\"features\": context_tokens, \"target\": target_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c9b097-be04-498b-8522-5c1be1b6e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ABCDataset(train_data)\n",
    "test_dataset = ABCDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc9ce130-ec5b-41b1-bea6-5ecb1a0fbc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[176, 225, 133, 65, 225, 133, 139, 90, 183, 65, 90, 183, 139, 48],\n",
       " [225, 133, 65, 225, 133, 51, 32, 78, 48],\n",
       " [176,\n",
       "  225,\n",
       "  133,\n",
       "  65,\n",
       "  225,\n",
       "  133,\n",
       "  139,\n",
       "  118,\n",
       "  70,\n",
       "  66,\n",
       "  58,\n",
       "  140,\n",
       "  90,\n",
       "  56,\n",
       "  95,\n",
       "  136,\n",
       "  77,\n",
       "  80,\n",
       "  291,\n",
       "  101,\n",
       "  183,\n",
       "  67,\n",
       "  182,\n",
       "  94,\n",
       "  46,\n",
       "  48],\n",
       " [67,\n",
       "  356,\n",
       "  231,\n",
       "  98,\n",
       "  225,\n",
       "  133,\n",
       "  65,\n",
       "  225,\n",
       "  133,\n",
       "  139,\n",
       "  225,\n",
       "  133,\n",
       "  65,\n",
       "  225,\n",
       "  133,\n",
       "  139,\n",
       "  48],\n",
       " [176, 90, 263, 65, 90, 263, 139, 90, 263, 65, 90, 263, 139, 48],\n",
       " [176, 90, 119, 65, 90, 119, 139, 90, 263, 65, 90, 263, 139, 48],\n",
       " [176,\n",
       "  90,\n",
       "  66,\n",
       "  119,\n",
       "  4,\n",
       "  66,\n",
       "  90,\n",
       "  66,\n",
       "  119,\n",
       "  484,\n",
       "  52,\n",
       "  90,\n",
       "  66,\n",
       "  119,\n",
       "  4,\n",
       "  66,\n",
       "  90,\n",
       "  66,\n",
       "  119,\n",
       "  362,\n",
       "  17,\n",
       "  8,\n",
       "  48],\n",
       " [150,\n",
       "  23,\n",
       "  153,\n",
       "  334,\n",
       "  47,\n",
       "  61,\n",
       "  90,\n",
       "  71,\n",
       "  23,\n",
       "  153,\n",
       "  334,\n",
       "  73,\n",
       "  90,\n",
       "  119,\n",
       "  139,\n",
       "  90,\n",
       "  119,\n",
       "  65,\n",
       "  90,\n",
       "  119,\n",
       "  139,\n",
       "  48],\n",
       " [176, 225, 133, 65, 225, 133, 139, 90, 133, 65, 90, 133, 139, 48],\n",
       " [176, 225, 133, 65, 225, 133, 139, 90, 183, 65, 90, 183, 139, 48],\n",
       " [176, 90, 183, 65, 90, 183, 139, 225, 133, 65, 225, 133, 139, 48],\n",
       " [176, 90, 133, 65, 90, 133, 139, 90, 183, 65, 90, 183, 139, 48],\n",
       " [176, 90, 263, 65, 90, 263, 139, 77, 78, 77, 332, 48],\n",
       " [176, 90, 119, 65, 90, 119, 139, 77, 78, 77, 332, 48],\n",
       " [176, 85, 89, 77, 119, 65, 101, 77, 119, 139, 90, 119, 65, 90, 119, 139, 48],\n",
       " [176, 90, 263, 65, 90, 263, 139, 90, 263, 65, 90, 263, 139, 48]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074b8afd-0a08-48e8-b654-177bf6caeb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([225, 133,  65,  ...,   0,   0,   0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b14b57-39fb-4ce4-ba6f-996e5f063de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target_dependency_mask(length):\n",
    "    full_mask = torch.ones(length, length)\n",
    "    ignore_mask = torch.tril(full_mask) < 1\n",
    "    full_mask.masked_fill_(ignore_mask, float('-inf'))\n",
    "    full_mask.masked_fill_(~ignore_mask, 0)\n",
    "    return full_mask\n",
    "\n",
    "def make_positional_encoding(max_length, embedding_size):\n",
    "    time = np.pi * torch.arange(0, max_length).float()\n",
    "    freq_dividers = torch.arange(1, embedding_size // 2 + 1).float()\n",
    "    inputs = time[:, None] / freq_dividers[None, :]\n",
    "    \n",
    "    result = torch.zeros(max_length, embedding_size)\n",
    "    result[:, 0::2] = torch.sin(inputs)\n",
    "    result[:, 1::2] = torch.cos(inputs)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a64846b2-06e8-4a07-902c-88cad6fcd041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_target_dependency_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca38f0c-72c3-4c4a-a2f1-8f4f51dbf9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_multihead_attention(queries, keys, values, padding_mask, dependency_mask,\n",
    "                           is_training=True,\n",
    "                           p_dropout=0):\n",
    "    \"\"\"\n",
    "    queries - BatchSize x HeadN x MaxLen x EmbeddingSize_Q\n",
    "    keys - BatchSize x HeadN x MaxLen x EmbeddingSize_K\n",
    "    values - BatchSize x HeadN x MaxLen x EmbeddingSize_V\n",
    "    \n",
    "    padding_mask - BatchSize x MaxLen\n",
    "    dependency_mask - MaxLen x MaxLen\n",
    "    \n",
    "    is_training - bool\n",
    "    weights_dropout - float\n",
    "    \n",
    "    result:\n",
    "        BatchSize x HeadN x MaxLen x EmbeddingSize_V\n",
    "    \"\"\"\n",
    "\n",
    "    # BatchSize x HeadN x MaxLen x MaxLen\n",
    "    relevances = torch.einsum('bhie,bhje->bhij', (queries, keys))\n",
    "    \n",
    "    # замаскировать элементы, выходящие за длины последовательностей ключей\n",
    "    padding_mask_expanded = padding_mask[:, None, :, None].expand_as(relevances)\n",
    "    relevances.masked_fill_(padding_mask_expanded, float('-inf'))\n",
    "    \n",
    "    # замаскировать пары <выходная позиция, входная позиция>\n",
    "    relevances = relevances + dependency_mask[None, None, :, :].expand_as(relevances)\n",
    "    \n",
    "    relevances = relevances * (1/(np.sqrt(keys.shape[3])))\n",
    "    normed_rels = F.softmax(relevances, dim=3)    \n",
    "    normed_rels = F.dropout(normed_rels, p_dropout, is_training)\n",
    "    \n",
    "    result = torch.einsum('bhti,bhie->bhte', (normed_rels, values))\n",
    "    \n",
    "    return result, normed_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9968e3-4431-47a3-8b08-a6b382fa7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class v_MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, embeddingSize, n_heads, dropout=0):\n",
    "        super().__init__()\n",
    "        assert embeddingSize % n_heads == 0, 'Размерность модели должна делиться нацело на количество голов'\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.queries_proj = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.keys_proj = nn.Linear(embeddingSize, embeddingSize)\n",
    "        self.values_proj = nn.Linear(embeddingSize, embeddingSize)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.last_attention_map = None\n",
    "    \n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        \"\"\"\n",
    "        sequence - BatchSize x Len x EmbeddingSize\n",
    "        padding_mask - BatchSize x Len\n",
    "        dependency_mask - Len x Len\n",
    "        \n",
    "        result - BatchSize x Len x EmbeddingSize\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size, max_len, embeddingSize = sequence.shape\n",
    "        \n",
    "        queries_flat = self.queries_proj(sequence)  # BatchSize x Len x EmbeddingSize\n",
    "        queries = queries_flat.view(batch_size, self.n_heads, max_len, -1)\n",
    "        \n",
    "        keys_flat = self.keys_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        keys = keys_flat.view(batch_size, self.n_heads, max_len, -1)\n",
    "        \n",
    "        values_flat = self.values_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        values = values_flat.view(batch_size, self.n_heads, max_len, -1)\n",
    "        \n",
    "        # BatchSize x Len x HeadsN x ValueSize\n",
    "        result, att_map = v_multihead_attention(queries, keys, values, padding_mask, dependency_mask, self.training, self.dropout)\n",
    "        result_flat = result.view(batch_size, max_len, embeddingSize)\n",
    "        \n",
    "        self.last_attention_map = att_map.detach()\n",
    "\n",
    "        return result_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f14eb900-d371-46d6-8a44-32eb1b06fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class v_TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embeddingSize, n_heads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = v_MultiheadSelfAttention(embeddingSize,\n",
    "                                                       n_heads,\n",
    "                                                       dropout=dropout)\n",
    "        self.first_dropout = nn.Dropout(dropout)\n",
    "        self.first_norm = nn.LayerNorm(embeddingSize)\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, embeddingSize),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.second_norm = nn.LayerNorm(embeddingSize)\n",
    "    \n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        att_features = self.self_attention(sequence, padding_mask, dependency_mask)\n",
    "\n",
    "        sequence = sequence + self.first_dropout(att_features)\n",
    "        sequence = self.first_norm(sequence)\n",
    "        \n",
    "        sequence = sequence + self.feedforward(sequence)\n",
    "        sequence = self.second_norm(sequence)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b79a95-783e-44ca-9c33-46ebe1cef999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class v_TransformerEncoder(nn.Module):\n",
    "    def __init__(self, n_layers, **kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            v_TransformerEncoderLayer(**kwargs)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        for layer in self.layers:\n",
    "            sequence = layer(sequence, padding_mask, dependency_mask)\n",
    "        return sequence\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2288ee47-bb25-4068-a3cb-60a7604c5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class musicModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, backbone, emb_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        self.backbone = backbone\n",
    "        self.out = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, seed_token_ids):\n",
    "        \"\"\"\n",
    "            seed_token_ids - BatchSize x MaxInLen\n",
    "        \"\"\"\n",
    "        batch_size, max_in_length = seed_token_ids.shape\n",
    "\n",
    "        seed_padding_mask = seed_token_ids == 0\n",
    "        dependency_mask = make_target_dependency_mask(max_in_length) \\\n",
    "            .to(seed_token_ids.device)\n",
    "        \n",
    "        seed_embs = self.embeddings(seed_token_ids)  # BatchSize x MaxInLen x EmbSize\n",
    "        pos_codes = make_positional_encoding(max_in_length,\n",
    "                                             self.embedding_size).unsqueeze(0).to(seed_embs.device)\n",
    "        seed_embs = seed_embs + pos_codes\n",
    "        seed_embs = self.emb_dropout(seed_embs)\n",
    "\n",
    "        # BatchSize x TargetLen x EmbSize\n",
    "        target_features = seed_embs\n",
    "        target_features = self.backbone(seed_embs,\n",
    "                                        dependency_mask=dependency_mask,\n",
    "                                        padding_mask=seed_padding_mask)\n",
    "        logits = self.out(target_features)  # BatchSize x TargetLen x VocabSize\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfdbda5e-3cea-4f80-b701-b4b1019e38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_cross_entropy(pred, target):\n",
    "    \"\"\"\n",
    "    pred - BatchSize x TargetLen x VocabSize\n",
    "    target - BatchSize x TargetLen\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\n",
    "    target_flat = target.view(-1)  # BatchSize*TargetLen\n",
    "    return F.cross_entropy(pred_flat, target_flat, ignore_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b3932e8-f377-4a91-8d72-23a7e5d7aa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 1640436\n"
     ]
    }
   ],
   "source": [
    "my_transf_model = ABCModel(tokenizer.vocab_size(),\n",
    "                                256,\n",
    "                                v_TransformerEncoder(\n",
    "                                    n_layers=3,\n",
    "                                    embeddingSize=256,\n",
    "                                    n_heads=16,\n",
    "                                    dim_feedforward=512,\n",
    "                                    dropout=0.1),\n",
    "                                emb_dropout=0.1)\n",
    "\n",
    "def get_params_number(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print('Количество параметров', get_params_number(my_transf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc8f16e1-5ee3-472e-be5e-fbd4c08bef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = my_transf_model.to(device)\n",
    "\n",
    "criterion = lm_cross_entropy\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=20,factor=0.5,verbose=True)\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a21ad98-e255-441c-8242-be8679f63941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch_i = 0\n",
    "best_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c7a60-4f08-47cc-a44f-d5f44be7173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n"
     ]
    }
   ],
   "source": [
    "epoch_n = 30\n",
    "\n",
    "for epoch_i in range(epoch_n):\n",
    "    epoch_start = datetime.datetime.now()\n",
    "    print('Эпоха {}'.format(epoch_i))\n",
    "\n",
    "    model.train()\n",
    "    mean_train_loss = 0\n",
    "    train_batches_n = 0\n",
    "    for batch_i, elem in enumerate(train_dataloader):\n",
    "        \n",
    "        batch_x = elem['features'].to(device)\n",
    "        batch_y = elem['target'].to(device)\n",
    "\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred, batch_y)\n",
    "\n",
    "        model.zero_grad ()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_train_loss += float(loss)\n",
    "        train_batches_n += 1\n",
    "\n",
    "    mean_train_loss /= train_batches_n\n",
    "    lr_scheduler.step(mean_train_loss)\n",
    "    \n",
    "    print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n",
    "                                                   (datetime.datetime.now() - epoch_start).total_seconds()))\n",
    "    print('Среднее значение функции потерь на обучении', mean_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c03721-e3a3-46ad-b0f5-8a535304fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mean_val_loss = 0\n",
    "val_batches_n = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_i, elem in enumerate(val_dataloader):\n",
    "        batch_x = elem['features'].to(device)\n",
    "        batch_y = elem['target'].to(device)\n",
    "\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred, batch_y)\n",
    "\n",
    "        mean_val_loss += float(loss)\n",
    "        val_batches_n += 1\n",
    "\n",
    "mean_val_loss /= val_batches_n\n",
    "print('Среднее значение функции потерь на валидации', mean_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6aa80-d34e-43d9-a295-3b130c34e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212d963-f907-4deb-b6ae-b198db2000ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(keys, notes) = read_abc(r\".\\yandex-music-generation-contest\\testset\\abc\\1.abc\")\n",
    "print(notes)\n",
    "bars = notes.split(\" | \")\n",
    "print(bars)\n",
    "notes_tokens = [tokenizer.encode(i + \" | \") for i in bars]\n",
    "print(notes_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31320e-8515-4a2d-b322-6109ca85564f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(keys, notes) = read_abc(r\".\\yandex-music-generation-contest\\testset\\abc\\1.abc\")\n",
    "\n",
    "t = tokenizer.encode(notes)\n",
    "t = torch.tensor(t).unsqueeze(0).to(device)\n",
    "print(t)\n",
    "g = []\n",
    "for i in range(100):\n",
    "    output = model(t)[0,-1].argmax()\n",
    "    t = torch.cat([t, output.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "    g += [int(t[-1][-1])]\n",
    "    \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fcb0f-6611-4038-b7e7-d30c33ae5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(list(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad532879-4217-4280-bde8-58b5d4f0c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "notes_tokens = [tokenizer.encode(i + \" | \") for i in bars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641f9ed-622f-4337-99d3-78d29e9ed175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class v_MusicCritic(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_latents, latent_dim, backbone, emb_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        self.backbone = backbone\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
    "        self.out = nn.Linear(embedding_size + 2, 1)\n",
    "    \n",
    "    def forward(self, offsets, note_ids, durations):\n",
    "        \"\"\"\n",
    "            note_ids - BatchSize x ChunkSize\n",
    "        \"\"\"\n",
    "        \n",
    "        note_embs = self.embeddings(note_ids)  # BatchSize x ChunkSize x EmbSize\n",
    "        offsets = torch.Tensor(offsets)\n",
    "        durations = torch.Tensor(durations)\n",
    "        durations = durations.unsqueeze(1)\n",
    "        durations = durations.unsqueeze(0)\n",
    "        offset = offset.unsqueeze(1)\n",
    "        offset = offset.unsqueeze(0)\n",
    "        embs = torch.cat([note_embs, durations], 2)\n",
    "        embs = torch.cat([note_embs, offset], 2)\n",
    "        \n",
    "        #note_embs = self.emb_dropout(note_embs)\n",
    "\n",
    "        # BatchSize x TargetLen x EmbSize\n",
    "        target_features = embs\n",
    "        target_features = self.backbone(embs)\n",
    "        logits = self.out(target_features)  # BatchSize x TargetLen x 1\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59f751-6d1b-4f69-b4dd-4a58e178f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embs = torch.Tensor([ [ [1,2,3], [4,5,6] ] ])\n",
    "offset = torch.Tensor([0, 0])\n",
    "offset = offset.unsqueeze(1)\n",
    "offset = offset.unsqueeze(0)\n",
    "\n",
    "torch.cat([embs, offset], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0d78a-5915-4315-9982-3c38e3da4310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
